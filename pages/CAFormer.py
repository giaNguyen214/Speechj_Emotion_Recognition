import streamlit as st

st.set_page_config(page_title="CAFormer", layout='wide')

language = st.selectbox(
    "Select Language",
    ("English", "Vietnamese")
)

# Sidebar má»¥c lá»¥c
st.sidebar.title("ğŸ“š Table of Contents")

st.sidebar.markdown("""
- [Transformer](#transformer)
- [Attention Mechanism](#attention-mechanism)
- [MetaFormer](#metaformer)
""", unsafe_allow_html=True)


if language == 'Vietnamese':
    st.markdown('<a id="transformer"></a>', unsafe_allow_html=True)
    st.title("Transformer trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn")
    st.write("Transformer thá»±c ra lÃ  Ä‘Æ°a vÃ o má»™t cÃ¢u vÃ  dá»± Ä‘oÃ¡n token káº¿ tiáº¿p cá»§a cÃ¢u Ä‘Ã³. Tháº­t ra, má»™t token khÃ´ng háº³n lÃ  má»™t word, nhÆ°ng Ä‘á»ƒ tiá»‡n, chÃºng ta sá»­ dá»¥ng 1 token ~ 1 word")
    st.write("MÃ¡y tÃ­nh chá»‰ cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c nhá»¯ng con sá»‘ mÃ  khÃ´ng hiá»ƒu Ä‘Æ°á»£c text mÃ  chÃºng ta nháº­p vÃ o. VÃ¬ váº­y, chÃºng ta cáº§n embed word thÃ nh nhá»¯ng vector. \
        Báº£n cháº¥t thÃ¬ má»—i tá»« vá»±ng cÅ©ng chá»‰ lÃ  cÃ¡ch chÃºng ta gÃ¡n ghÃ©p quy luáº­t cho vá»‹ trÃ­ Ä‘áº·t tá»« vÃ  ngá»¯ nghÄ©a Ä‘i kÃ¨m. Náº¿u thay vÃ¬ ghi 'h' lÃ  'h' thÃ¬ ta cÃ³ thá»ƒ Ä‘áº·t 'h' lÃ  1, tÆ°Æ¡ng tá»±, 'e' 2, 'l' 3, 'o' 4 thÃ¬ 'hello' sáº½ lÃ  12334 \
        NhÆ°ng vÃ¬ sá»‘ lÆ°á»£ng tá»« vá»±ng vÃ  ngá»¯ nghÄ©a nhiá»u, nÃªn khÃ´ng thá»ƒ biá»ƒu diá»…n dÆ°á»›i dáº¡ng nhá»¯ng con sá»‘ Ä‘Æ¡n giáº£n nhÆ° váº­y Ä‘Æ°á»£c mÃ  cáº§n Ä‘áº¿n nhá»¯ng high-dimension vectors -> ÄÃ³ lÃ  lÃ½ do ra Ä‘á»i cá»§a Embedding") 
    st.write("LÆ°u Ã½ lÃ  trong GPT, há» tÃ¡ch ra thÃ nh tá»«ng bá»™ vá»›i chá»©c nÄƒng riÃªng nhÆ° Token Embedding, Positional Embedding, Querry, Key, Value,...")
 
    st.write("Sau khi Ä‘Ã£ train vÃ  cÃ³ Ä‘Æ°á»£c má»™t model Embedding, chÃºng ta Ä‘Æ°a cÃ¡c tokens vÃ o vÃ  láº¥y ra Ä‘Æ°á»£c embeddings tÆ°Æ¡ng á»©ng. Tiáº¿p Ä‘Ã³, chÃºng ta cáº§n Position embedding, náº¿u khÃ´ng thÃ¬ 'I love you' vÃ  'You love I' sáº½ lÃ  chung 1 vector set. ThÃªm vÃ o Ä‘Ã³, bá»Ÿi vÃ¬ Embedding model Ä‘Æ°á»£c huáº¥n luyá»‡n chung nÃªn lÃ  cÃ¡c embeddings Ä‘áº§u ra sáº½ khÃ´ng cÃ³ thÃ´ng tin ngá»¯ cáº£nh cá»§a cÃ¢u. \
    VÃ­ dá»¥: The football match was exciting. (match: a sport event) and He lit the fire with a match. (match: a small stick made of wood or cardboard that is used for lighting a fire, cigarette, etc.). Cáº£ 2 tá»« match Ä‘á»u cÃ¹ng lÃ  noun, nhÆ°ng Ã½ nghÄ©a khÃ¡c nhau rÃµ rá»‡t. Máº·c dÃ¹ váº­y, khi Ä‘Æ°a qua Embedding model thÃ¬ nÃ³ cho ra cÃ¹ng má»™t káº¿t quáº£. Äiá»u nÃ y lÃ  khÃ´ng Ä‘Ãºng")

    st.write("VÃ¬ váº­y, Ã½ nghÄ©a cá»§a Transformer lÃ  Ä‘iá»u chá»‰nh cÃ¡c embeddings Ä‘Ã³ theo context Ä‘á»ƒ táº¡o nÃªn má»™t embedding khÃ¡c giÃ u thÃ´ng tin ngá»¯ cáº£nh hÆ¡n => Attention mechanism")
    st.info("Position embedding lÃ  cho biáº¿t vá»‹ trÃ­ cá»§a token trong cÃ¢u, cÃ²n Attention lÃ  Ä‘á»ƒ biáº¿t quan há»‡ giá»¯a cÃ¡c tokens.")


    st.markdown('<a id="attention-mechanism"></a>', unsafe_allow_html=True)
    st.title("Attention mechanism")
    st.write("Äá»ƒ Ä‘Æ°a thÃ´ng tin cá»§a cÃ¡c embeddings cá»§a cÃ¡c tokens khÃ¡c vÃ o trong, cÃ³ má»™t váº¥n Ä‘á» cáº§n quan tÃ¢m chÃ­nh lÃ  trá»ng sá»‘ lÃ  bao nhiÃªu. NghÄ©a lÃ  embedding khÃ¡c áº£nh hÆ°á»Ÿng Ä‘áº¿n target embedding vá»›i má»©c Ä‘á»™ nÃ o. Bá»Ÿi vÃ¬ cÃ³ nhá»¯ng tá»« ráº¥t quan trá»ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh context information, nhÆ°ng cÃ³ má»™t sá»‘ tá»« khÃ´ng liÃªn quan.")
    st.write("Äá»ƒ tÃ­nh cÃ¡c trá»ng sá»‘ nÃ y, chÃºng ta dÃ¹ng Ä‘áº¿n Querry, Key vÃ  Value. VÃ­ dá»¥, á»Ÿ má»™t token, chÃºng ta cáº§n nhÃ¬n xem xung quanh token Ä‘Ã³ Ä‘ang cÃ³ nhá»¯ng tá»« ngá»¯ gÃ¬ Ä‘á»ƒ bá»• sung ngá»¯ nghÄ©a cho chÃ­nh báº£n thÃ¢n token hiá»‡n Ä‘ang xÃ©t. VÃ­ dá»¥ náº¿u Ä‘Ã³ lÃ  danh tá»« thÃ¬ cÃ³ thá»ƒ sáº½ tÃ¬m kiáº¿m cÃ¡c tÃ­nh tá»«. NhÆ°ng mÃ¡y tÃ­nh khÃ´ng thá»ƒ hiá»ƒu Ä‘Æ°á»£c tháº¿ nhÆ° con ngÆ°á»i chÃºng ta, nÃªn cáº§n má»™t model Ä‘á»ƒ táº¡o ra cÃ¢u há»i Ä‘áº¡i loáº¡i nhÆ° kiá»ƒu: What information do I need to request from other tokens?. Vai trÃ² cá»§a Querry lÃ  sinh ra cÃ¢u há»i cho chÃºng ta.")
    st.write("TÆ°Æ¡ng tá»±, Key chÃ­nh lÃ  cÃ¢u tráº£ lá»i cá»§a Querry. CÃ³ thá»ƒ hiá»ƒu Key nhÆ° má»™t báº£n mÃ´ táº£ ngáº¯n vá» thÃ´ng tin cá»§a token, token giá»›i thiá»‡u mÃ¬nh Ä‘ang cÃ³ nhá»¯ng gÃ¬ (khÃ´ng Ä‘Æ°a thÃ´ng tin, ná»™i dung cá»¥ thá»ƒ cá»§a token). Sau Ä‘Ã³, tÃ­nh dot product giá»¯a querry vÃ  key vÃ  Ä‘Æ°a vÃ o softmax Ä‘á»ƒ láº¥y ra káº¿t quáº£ cuá»‘i cÃ¹ng, Ä‘Ã¢y chÃ­nh lÃ  attention scores cho táº¥t cáº£ cÃ¡c tokens khÃ¡c Ä‘á»‘i vá»›i target token. ChÃºng thá»ƒ hiá»‡n má»©c Ä‘á»™ Ä‘Ã³ng gÃ³p cá»§a cÃ¡c tokens Ä‘Ã³ vÃ o token hiá»‡n táº¡i")
    st.write("Tuy nhiÃªn, ná»™i dung thá»±c sá»± mÃ  chÃºng ta cáº§n láº¥y tá»« cÃ¡c tokens khÃ´ng pháº£i lÃ  embedding gá»‘c ban Ä‘áº§u cá»§a chÃºng mÃ  lÃ  Value (má»™t bÆ°á»›c nhÃºng embedding gá»‘c Ä‘á»ƒ sinh ra embedding má»›i chá»©a gá»n giÃ¡ trá»‹ cá»§a token)")
    st.write("Khi mÃ  chÃºng ta tÃ­nh Querry-Key thÃ¬ thá»±c cháº¥t nÃ³ Ä‘Ã£ bao gá»“m token hiá»‡n táº¡i vÃ o rá»“i vÃ  thÆ°á»ng thÃ¬ trá»ng sá»‘ (attention scores) cá»§a nÃ³ sáº½ lá»›n hÆ¡n so vá»›i cÃ¡c tokens khÃ¡c.")
    st.write("ThÃªm ná»¯a lÃ  chÃºng ta cáº§n masking. VÃ¬ bÃ i toÃ¡n lÃ  dá»± Ä‘oÃ¡n tá»« tiáº¿p theo dá»±a trÃªn thÃ´ng tin cÃ¢u Ä‘Ã£ Ä‘Æ°á»£c cho trÆ°á»›c, nghÄ©a lÃ  chÃºng ta sáº½ khÃ´ng biáº¿t Ä‘Æ°á»£c Ä‘áº±ng sau token nÃ y sáº½ cÃ³ nhá»¯ng gÃ¬ cho nÃªn chá»‰ tá»•ng há»£p thÃ´ng tin cá»§a cÃ¡c tokens trÆ°á»›c Ä‘Ã³ vÃ o target token thÃ´i")
    st.image("images/querry-key.jpg")
    st.info("Nhá»¯ng gÃ¬ á»Ÿ trÃªn Ä‘Ã¢y lÃ  self-attention. CÃ²n cross-attention lÃ  2 loáº¡i data sets khÃ¡c nhau. VÃ­ dá»¥ nhÆ° giá»¯a tiáº¿ng Anh vÃ  tiáº¿ng Viá»‡t, giá»¯a audio waveform vÃ  transcript cá»§a nÃ³. Äiá»ƒm khÃ¡c biá»‡t á»Ÿ cross attention lÃ  tÃ­nh querry á»Ÿ dataset nÃ y vÃ  key á»Ÿ dataset kia, cÃ²n láº¡i lÃ m nhÆ° self-attention")
    st.info("Multi-headed attention lÃ  dÃ¹ng nhiá»u bá»™ Q, K, V khÃ¡c nhau Ä‘á»ƒ Ä‘Æ°a vÃ o cÃ¡c single head self-attention Ä‘Ã£ trÃ¬nh bÃ y Ä‘á»ƒ Ä‘Æ°a ra cÃ¡c output khÃ¡c nhau, sau Ä‘Ã³ ná»‘i cÃ¡c outputs láº¡i vÃ  cÃ³ thá»ƒ Ä‘Æ°a qua má»™t linear layer Ä‘á»ƒ gá»™p thÃ´ng tin vÃ  Ä‘Æ°a vá» Ä‘Ãºng sá»‘ chiá»u mong muá»‘n")

    col1, col2 = st.columns(2)
    with col1:
        st.image('images/attention.jpg')
    with col2:
        st.image('images/mlp.jpg')
    st.write("NhÃ¬n vÃ o Ä‘Ã¢y chÃºng ta cÃ³ thá»ƒ tháº¥y sá»± khÃ¡c biá»‡t rÃµ rÃ ng giá»¯a Attention vÃ  MLP truyá»n thá»‘ng. á» Attention layer lÃ  cÃ¡c embedding inputs trao Ä‘á»•i thÃ´ng tin vá»›i nhau táº¡o ra cÃ¡c embedding outputs. Output cá»§a má»™t token phá»¥ thuá»™c vÃ o nhiá»u tokens khÃ¡c. CÃ²n á»Ÿ MLP thÃ¬ embedding output cá»§a má»™t token chá»‰ phá»¥ thuá»™c vÃ o duy nháº¥t embedding input cá»§a token Ä‘Ã³")


    st.markdown('<a id="metaformer"></a>', unsafe_allow_html=True)
    st.title("MetaFormer for Visual task")

    st.write("**MetaFormer** lÃ  kiáº¿n trÃºc chung cho cáº£ thá»‹ giÃ¡c mÃ¡y tÃ­nh vÃ  xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, vá»›i hai thÃ nh pháº§n tÃ¡ch biá»‡t: **Token Mixer** (xá»­ lÃ½ tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c vá»‹ trÃ­) vÃ  **Channel MLP** (xá»­ lÃ½ tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c kÃªnh).")

    # Liá»‡t kÃª biáº¿n thá»ƒ
    st.write("""
    **Bá»‘n biáº¿n thá»ƒ chÃ­nh**:
    1. **IdentityFormer** â€“ khÃ´ng cÃ³ mixer, baseline.  
    2. **RandFormer** â€“ mixer ngáº«u nhiÃªn.  
    3. **ConvFormer** â€“ sá»­ dá»¥ng convolution lÃ m mixer cá»¥c bá»™.  
    4. **CAFormer** â€“ sá»­ dá»¥ng attention lÃ m mixer toÃ n cá»¥c.  
    Trong Ä‘Ã³, IdentityFormer vÃ  RandFormer chá»‰ lÃ  2 model Ä‘Æ¡n giáº£n Ä‘á»ƒ chá»©ng minh tÃ­nh kháº£ thi cá»§a kiáº¿n trÃºc.
    """)

    # HÃ¬nh áº£nh
    _, c2, _ = st.columns([3,1,3])
    with c2:
        st.image("images/metaformer.jpg", caption='Cáº¥u trÃºc chung cá»§a MetaFormer')
    _, c2, _ = st.columns([1,3,1])
    with c2:
        st.image("images/metaformer_benchmark.jpg", caption='MetaFormer benchmark')
    # Pseudo-code block
    st.markdown("**CÃ´ng thá»©c cá»§a má»™t block MetaFormer**:")
    st.code("""
    # I: input
    X     = InputEmbedding(I)
    X'    = X + TokenMixer( Norm1(X) )  # residual + token mixing
    X''   = X' + MLP_ch( Norm2(X') )    # residual + channel MLP
    """, language="python")

    # So sÃ¡nh CNN vÃ  Transformer
    st.write("""
    - **CNN**: lÃ  **local mixer**, trá»™n thÃ´ng tin trong neighborhood pixel.  
    - **Transformer**: lÃ  **global mixer**, attention tá»•ng há»£p ngá»¯ cáº£nh tá»« má»i token.  
    """)

    # Shape issue
    st.info(
        "MetaFormer xá»­ lÃ½ viá»‡c báº¥t Ä‘á»“ng bá»™ shape giá»¯a cÃ¡c stage, mixer khÃ¡c nhau báº±ng cÃ¡ch Ä‘Æ¡n giáº£n lÃ  **reshape** dá»¯ liá»‡u giá»¯a cÃ¡c stage: "
        "tá»« H x W x C (CNN) â†’ (HÂ·W) x C (Attention) hoáº·c ngÆ°á»£c láº¡i tÃ¹y vÃ o Token Mixer"
    )
    st.write("MetaFormer chia thÃ nh 4 giai Ä‘oáº¡n. Vá»›i input Ä‘áº§u vÃ o lÃ  áº£nh vá»›i shape H x W x 3. VÃ  cÃ¡c káº¿t quáº£ á»Ÿ stage tiáº¿p theo:")
    _, c2, _ = st.columns(3)    
    with c2:
        st.image("images/stages.jpg")

    st.title("ConvFormer")
    st.write("á» model nÃ y, tÃ¡c giáº£ sá»­ dá»¥ng hoÃ n toÃ n lÃ  CNN-based block, trong Ä‘Ã³ má»—i block Ä‘Æ°á»£c thiáº¿t káº¿ theo kiáº¿n trÃºc MetaFormer nhÆ° Ä‘Ã£ trÃ¬nh bÃ y á»Ÿ trÃªn, vÃ  token mixer há» sá»­ dá»¥ng lÃ  convolution.")
    _, c2, _ = st.columns(3)
    with c2:
        st.image("images/convformer.jpg", caption="Overall ConvFormer framework")
    _, c2, _ = st.columns([3, 1, 3])
    with c2:
        st.image("images/convformer_block.jpg", caption="ConvFormer block")
        
    with st.expander("Depthwise separable convolution"):
        st.write("ÄÃ¢y lÃ  cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a CNN bÃ¬nh thÆ°á»ng. ChÃºng ta sáº½ cÃ³ nhiá»u filter (trong hÃ¬nh dÆ°á»›i Ä‘Ã¢y lÃ  8), vÃ  láº§n lÆ°á»£t Ä‘Æ°a input qua tá»«ng filter Ä‘á»ƒ ra káº¿t quáº£. Váº¥n Ä‘á» lÃ  náº¿u kÃ­ch thÆ°á»›c input lá»›n thÃ¬ filter size lá»›n theo (filter size hiá»‡n táº¡i lÃ  8x3x3), dáº«n Ä‘áº¿n tiÃªu tá»‘n tÃ i nguyÃªn vÃ  thá»i gian tÃ­nh toÃ¡n")
        _, c2, _ = st.columns([1, 3, 1])
        with c2:
            st.image("images/normal_cnn.jpg", caption="Normal CNN")
        
        st.write("Do Ä‘Ã³, thay vÃ¬ chÃºng ta dÃ¹ng nhiá»u filter size lá»›n (8x3x3), má»—i filter duyá»‡t trÃªn toÃ n bá»™ channels cá»§a input, chÃºng ta tÃ¡ch riÃªng thÃ nh cÃ¡c filter cÃ³ kÃ­ch thÆ°á»›c nhá» láº¡i (4x3x3) vÃ  chá»‰ duyá»‡t trÃªn 1 channel cá»§a input. Tuy sá»‘ lÆ°á»£ng filter giá»¯ nguyÃªn nhÆ°ng kÃ­ch thÆ°á»›c vÃ  sá»‘ lÆ°á»£ng tham sá»‘ giáº£m Ä‘i Ä‘Ã¡ng ká»ƒ.")
        _, c2, _ = st.columns([1, 3, 1])    
        with c2:
            st.image("images/depthwise.jpg", caption="Depthwise convolution")
        
        st.write("ChÃºng ta tháº­m chÃ­ cÃ³ thá»ƒ tÃ¡ch ra nhiá»u channels hÆ¡n ná»¯a, giá»‘ng vÃ­ dá»¥ dÆ°á»›i Ä‘Ã¢y. Tuy nhiÃªn, má»™t váº¥n Ä‘á» vá»›i Depthwise convolution chÃ­nh lÃ  má»—i layer cá»§a output chá»‰ lÃ  tá»•ng há»£p thÃ´ng tin cá»§a duy nháº¥t 1 channel cá»§a input thÃ´i (pháº§n mÃ u tráº¯ng trong hÃ¬nh áº£nh dÆ°á»›i). Äiá»u nÃ y khÃ´ng Ä‘Ãºng so vá»›i CNN-based bÃ¬nh thÆ°á»ng khi mÃ  má»—i output pixel lÃ  tá»•ng há»£p cá»§a nhá»¯ng pixels lÃ¢n cáº­n.")
        _, c2, _ = st.columns([1, 3, 1])    
        with c2:
            st.image("images/depthwise_problem.jpg", caption="Depthwise convolution problem")
        
        st.write("Do Ä‘Ã³, sau má»—i depthwise, chÃºng ta cáº§n filter dáº¡ng pointwise Ä‘á»ƒ thÃ´ng tin giá»¯a cÃ¡c channels cÃ³ thá»ƒ trao Ä‘á»•i Ä‘Æ°á»£c vá»›i nhau. Káº¿t há»£p Depthwise + Pointwise convolution láº¡i nhÆ° tháº¿ chÃºng ta sáº½ cÃ³ Ä‘Æ°á»£c Depthwise separable convolution.")
        _, c2, _ = st.columns([1, 3, 1])    
        with c2:
            st.image("images/pointwise.jpg", caption="Pointwise convolution")

    st.title("CAFormer")
    st.write("TÆ°Æ¡ng tá»± nhÆ° ConvFormer, chá»‰ khÃ¡c lÃ  tÃ¡c giáº£ muá»‘n káº¿t há»£p cáº£ local vÃ  global representation. Tuy nhiÃªn, náº¿u Ã¡p dá»¥ng attention Ä‘á»ƒ há»c global representation ngay tá»« input Ä‘áº§u vÃ o thÃ¬ kÃ­ch thÆ°á»›c áº£nh lá»›n, sáº½ tiÃªu tá»‘n tÃ i nguyÃªn nhiá»u. VÃ¬ tháº¿, há» Ä‘Ã£ Ä‘Æ°a input vÃ o CNN Ä‘á»ƒ tá»•ng há»£p local information (2 stages Ä‘áº§u), sau Ä‘Ã³ reshape vÃ  Ä‘Æ°a vÃ o Attention Ä‘á»ƒ há»c global.")
    _, c2, _ = st.columns(3)
    with c2:
        st.image("images/caformer.jpg", caption="Overall CAFormer framework")
    _, c2, _ = st.columns([3, 1, 3])
    with c2:
        st.image("images/caformer_block.jpg", caption="CAFormer block")
else: 
    # Transformer
    st.markdown('<a id="transformer"></a>', unsafe_allow_html=True)
    st.title("Transformer in natural language processing")
    st.write("In practice, a Transformer model actually takes in a sentence and predicts its next token (A token isnâ€™t exactly a word, but for simplicity we treat 1 token â‰ˆ 1 word)")
    st.write(
        "At first, because computers only understand numbers, not text, so we need to embed words into vectors. "
        "The meaning behind embedding word/ character to numbers is that each vocabulary term is essentially our rule to map a character to a numeric position plus semantic meaning. "
        "For instance, if we mapped 'h'â†’1, 'e'â†’2, 'l'â†’3, 'o'â†’4, then 'hello' would become '12334'. "
        "But because vocabularies are huge and semantics complex, we cannot use simple numbers to represent the word. So that's why high-dimensional embeddings were invented. Therefore, 'hello' can be '1234...863"
    )

    st.write(
        "After training a standalone embedding model, we feed in tokens and get their embeddings. "
        "Then we need positional embeddings. Without them, 'I love you' and 'You love I' map to the same vectors. "
        "Moreover, standalone embeddings lack sentenceâ€level context. "
        "For example, â€œThe football match was exciting.â€ (match: a sport event) vs â€œHe lit the fire with a match.â€ "
        "(match: a small stick for lighting a fire). Both are nouns but have very different meanings â€” yet a raw shared embedding model would treat them the same, which is incorrect."
    )

    st.write(
        "Therefore, the Transformerâ€™s job is to adjust those embeddings according to context, yielding richer, contextâ€aware embeddings â†’ the Attention mechanism."
    )
    st.info("Positional embeddings encode each tokenâ€™s position in the sentence; Attention encodes relationships between tokens.")

    # Attention Mechanism
    st.markdown('<a id="attention-mechanism"></a>', unsafe_allow_html=True)
    st.title("Attention Mechanism")
    st.write(
        "To bring information from other tokensâ€™ embeddings into a target token, we must decide how much each should contribute. "
        "Some tokens are crucial for context; others are irrelevant."
    )
    st.write(
        "We compute these weights via Query, Key, and Value. For each token, we examine other tokens to enrich its meaning. "
        "For humans, if the token is a noun, we might look for adjectives. Computers canâ€™t do that intuitively, so the Query module asks: "
        "'What information do I need to request from other tokens?' That is the role of the Query."
    )
    st.write(
        "Similarly, the Key is the Queryâ€™s answerâ€”itâ€™s a brief description of each token (what it has), without revealing full content. "
        "We then take the dot product of Query and Key and apply softmax to get attention scores: each tokenâ€™s contribution to the target."
    )
    st.write(
        "And we multiply the attention score with the content of corresponding token's embeddings. However, the actual content we want isnâ€™t the tokenâ€™s original embedding but its Value (a projection that compactly represents the token)."
    )
    st.write(
        "When we compute QueryÂ·Key, the target token is naturally considered, so its self-attention score is usually higher. "
        "We also apply masking. Since we predict the next token, we cannot see future tokens, so we only attend to past tokens."
    )
    st.image("images/querry-key.jpg")
    st.info(
        "All of the above is self-attention. Cross-attention uses Query from one dataset and Key from another "
        "(e.g., Englishâ†”Vietnamese text or audio waveformâ†”transcript). Everything else works the same."
    )
    st.info(
        "Multi-head attention runs multiple Q/K/V sets in parallel through singleâ€head attention modules, "
        "concatenates their outputs, and then optionally projects back to the desired dimension via a linear layer."
    )

    col1, col2 = st.columns(2)
    with col1:
        st.image('images/attention.jpg', caption="Self-Attention vs MLP")
    with col2:
        st.image('images/mlp.jpg',      caption="MLP Layer")

    st.write(
        "Here we can see the clear difference between Attention and a traditional MLP. "
        "In an Attention layer, all input embeddings exchange information to form output embeddingsâ€”"
        "each output token depends on many input tokens. "
        "In an MLP, each output token depends only on its own input embedding."
    )
    
    # MetaFormer
    st.markdown('<a id="metaformer"></a>', unsafe_allow_html=True)
    st.title("MetaFormer for Visual Tasks")

    st.write("**MetaFormer** is a general architecture for both computer vision and natural language processing, with two separate components: **Token Mixer** (handling interactions between spatial positions) and **Channel MLP** (handling interactions between channels).")

    # List variants
    st.write("""
    **Four main variants**:
    1. **IdentityFormer** â€“ no mixer, baseline.  
    2. **RandFormer** â€“ random mixer.  
    3. **ConvFormer** â€“ uses convolution as a local mixer.  
    4. **CAFormer** â€“ uses attention as a global mixer.  
    Among these, IdentityFormer and RandFormer are simple models used mainly to demonstrate the viability of the architecture.
    """)

    # Images
    _, c2, _ = st.columns([3,1,3])
    with c2:
        st.image("images/metaformer.jpg", caption='General structure of MetaFormer')
    _, c2, _ = st.columns([1,3,1])
    with c2:
        st.image("images/metaformer_benchmark.jpg", caption='MetaFormer benchmark')

    # Pseudo-code block
    st.markdown("**Formula of a MetaFormer block**:")
    st.code("""
    # I: input
    X     = InputEmbedding(I)
    X'    = X + TokenMixer( Norm1(X) )  # residual + token mixing
    X''   = X' + MLP_ch( Norm2(X') )    # residual + channel MLP
    """, language="python")

    # Compare CNN and Transformer
    st.write("""
    - **CNN**: a **local mixer**, mixing information within neighboring pixels.  
    - **Transformer**: a **global mixer**, aggregating context from all tokens using attention.  
    """)

    # Shape issue
    st.info(
        "MetaFormer handles the mismatch in shape between different stages and different mixers simply by **reshaping** data between stages: "
        "from H x W x C (CNN format) â†’ (HÂ·W) x C (Attention format) or vice versa, depending on the Token Mixer."
    )
    st.write("MetaFormer divides processing into four stages. The input is an image with shape H x W x 3, and the outputs at each following stage are:")
    _, c2, _ = st.columns(3)    
    with c2:
        st.image("images/stages.jpg")

    st.title("ConvFormer")
    st.write("In this model, the authors use an entirely CNN-based block design, where each block follows the MetaFormer structure described above, and the token mixer they use is convolution.")
    _, c2, _ = st.columns(3)
    with c2:
        st.image("images/convformer.jpg", caption="Overall ConvFormer framework")
    _, c2, _ = st.columns([3, 1, 3])
    with c2:
        st.image("images/convformer_block.jpg", caption="ConvFormer block")

    with st.expander("Depthwise separable convolution"):
        st.write("Here's how a normal CNN works. We have multiple filters (8 in the example below), and the input passes through each filter to produce outputs. The issue is that if the input size is large, the filter size also grows (current filter size: 8x3x3), leading to high resource consumption and slow computation.")
        _, c2, _ = st.columns([1, 3, 1])
        with c2:
            st.image("images/normal_cnn.jpg", caption="Normal CNN")
        
        st.write("Therefore, instead of using large filter sizes (8x3x3) with each filter spanning across all input channels, we split them into smaller filters (4x3x3) that only operate on a single channel. Although the number of filters stays the same, the size and number of parameters are significantly reduced.")
        _, c2, _ = st.columns([1, 3, 1])    
        with c2:
            st.image("images/depthwise.jpg", caption="Depthwise convolution")
        
        st.write("We can even split into more channels, as shown in the example below. However, a problem with depthwise convolution is that each output layer only aggregates information from one input channel (illustrated by the white area in the image below). This is unlike a standard CNN where each output pixel aggregates information from neighboring pixels across channels.")
        _, c2, _ = st.columns([1, 3, 1])    
        with c2:
            st.image("images/depthwise_problem.jpg", caption="Depthwise convolution problem")
        
        st.write("Thus, after each depthwise convolution, we need a pointwise convolution to allow information exchange between channels. Combining depthwise and pointwise convolution gives us what is called Depthwise Separable Convolution.")
        _, c2, _ = st.columns([1, 3, 1])    
        with c2:
            st.image("images/pointwise.jpg", caption="Pointwise convolution")

    st.title("CAFormer")
    st.write("Similar to ConvFormer, but here the authors aim to combine both local and global representations. However, if attention is used to capture global information right from the input stage, the large image size would lead to high resource consumption. Therefore, they first pass the input through CNNs to gather local information (in the first 2 stages), then reshape and apply Attention to learn global representations.")
    _, c2, _ = st.columns(3)
    with c2:
        st.image("images/caformer.jpg", caption="Overall CAFormer framework")
    _, c2, _ = st.columns([3, 1, 3])
    with c2:
        st.image("images/caformer_block.jpg", caption="CAFormer block")
